# Data Quality and Preprocessing Analysis
This repository contains a comprehensive analysis of data quality dimensions and preprocessing techniques applied to a sample dataset. The analysis covers various aspects of data quality, including accuracy, completeness, consistency, timeliness, validity, and integrity. Additionally, it demonstrates how to handle different types of data errors, such as missing values, duplicate data, inconsistent data, and outliers. The preprocessing steps include feature scaling through normalization and standardization.

### Data Quality Dimensions

    Accuracy: Correctness and precision of the data.
    Completeness: Extent to which data is free from missing values or gaps.
    Consistency: Absence of contradictions within the data.
    Timeliness: Relevance and currency of the data.
    Validity: Adherence to predefined rules and constraints.
    Integrity: Overall reliability and trustworthiness of the data.

### Types of Data Errors

    Missing Data: Data values that are not recorded or are unavailable.
    Duplicate Data: Replicated data entries within the dataset.
    Inconsistent Data: Discrepancies or contradictions in data values.
    Outliers: Data points that deviate significantly from the overall distribution.

### Feature Scaling

Feature scaling techniques are used to normalize or standardize data to prepare it for analysis and modeling:

  Normalization: Scales data to a specific range, typically between 0 and 1.
  
  Standardization: Transforms data to have a mean of 0 and a standard deviation of 1.

### Dataset

The dataset used in this repository is from the California Housing dataset. It contains information on housing prices in California, with attributes such as longitude, latitude, housing median age, total rooms, and more.
